{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b71a43f",
   "metadata": {},
   "source": [
    "# Lab 1: Implementing Logistic Regression Components from Scratch\n",
    "\n",
    "## 1. Objectives\n",
    "In this notebook, we will implement the fundamental mathematical building blocks of a neural network using only **NumPy**. This will help in understanding how backpropagation and optimization work under the hood.\n",
    "\n",
    "## 2. Mathematical Background\n",
    "We will focus on the following three components:\n",
    "1.  **Sigmoid Activation Function**: Maps any real-valued number into the range (0, 1).\n",
    "2.  **Binary Cross-Entropy Loss**: Measures the performance of a classification model.\n",
    "3.  **Gradient Descent**: The optimization algorithm used to minimize the loss by updating weights.\n",
    "\n",
    "## 3. Implementation Challenge\n",
    "\n",
    "### Task 1: The Sigmoid Function\n",
    "The formula is:\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "**Question for you:** In terms of deep learning, why do we use `Sigmoid` instead of a simple linear function for the output of a binary classifier? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49e5de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bb82fe",
   "metadata": {},
   "source": [
    "### Analysis of Sigmoid Implementation\n",
    "- **Domain**: $(-\\infty, \\infty)$\n",
    "- **Range**: $(0, 1)$\n",
    "- **Interpretation**: Represents the probability $P(y=1|x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fa55ca",
   "metadata": {},
   "source": [
    "In the naive implementation of Sigmoid, `np.exp(-z)` can overflow if $z$ is a large negative number (e.g., $z = -1000$). To prevent this, we use the **stable sigmoid** approach:\n",
    "\n",
    "- If $z \\ge 0$: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "- If $z < 0$: $\\sigma(z) = \\frac{e^z}{1 + e^z}$\n",
    "\n",
    "This ensures that we only compute $e^x$ where $x \\le 0$, keeping the exponential term bounded between $(0, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afe6b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_sigmoid(z):\n",
    "    # For z >= 0: 1 / (1 + exp(-z))\n",
    "    # For z < 0: exp(z) / (1 + exp(z))\n",
    "    # This ensures that exp() is only called with non-positive arguments.\n",
    "    return np.where(z >= 0, 1 / (1 + np.exp(-z)), np.exp(z) / (1 + np.exp(z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a924547e",
   "metadata": {},
   "source": [
    "\n",
    "### Task 2: Binary Cross-Entropy Loss\n",
    "The formula for a single sample is:\n",
    "$$L = -(y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y}))$$\n",
    "\n",
    "**Question for you:** If the model predicts $\\hat{y} = 0.9$ and the true label is $y = 1$, will the loss be high or low? What happens to the math if $\\hat{y}$ is exactly $0$ or $1$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c133e42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the Binary Cross-Entropy (BCE) between true labels and predictions.\n",
    "\n",
    "    Args:\n",
    "        y_true: Ground truth values.\n",
    "        y_pred: Predicted values from the model.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15\n",
    "    # np.clip:Clip array elements to the specified min and max range\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon) \n",
    "    loss = -(y_true * np.log(y_pred) + (1-y_true) * np.log(1 - y_pred))\n",
    "    # return the average loss over the batch\n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345b1490",
   "metadata": {},
   "source": [
    "## Task 3: Gradient Descent and The Chain Rule Derivation\n",
    "\n",
    "To understand why we update weights using the formula $dw = \\frac{1}{n} X^T(\\hat{y} - y)$, we must look at the **Chain Rule**.\n",
    "\n",
    "### 1. The Components\n",
    "Logistic Regression follows this flow:\n",
    "$$w, b \\xrightarrow{\\text{Linear}} z = wx + b \\xrightarrow{\\text{Sigmoid}} \\hat{y} = \\sigma(z) \\xrightarrow{\\text{BCE}} Loss$$\n",
    "\n",
    "### 2. The Chain Rule Formula\n",
    "To find how the Loss changes with respect to weights ($w$):\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}$$\n",
    "\n",
    "### 3. Step-by-Step Differentiation\n",
    "* **Step 1 (Loss to Prediction):** Differentiating $L = -(y \\log\\hat{y} + (1-y) \\log(1-\\hat{y}))$ gives:\n",
    "    $$\\frac{\\partial L}{\\partial \\hat{y}} = \\frac{\\hat{y}-y}{\\hat{y}(1-\\hat{y})}$$\n",
    "* **Step 2 (Prediction to Linear):** Differentiating the Sigmoid function $\\hat{y} = \\sigma(z)$ gives:\n",
    "    $$\\frac{\\partial \\hat{y}}{\\partial z} = \\hat{y}(1-\\hat{y})$$\n",
    "* **Step 3 (Linear to Weights):** Differentiating $z = wx + b$ with respect to $w$ gives:\n",
    "    $$\\frac{\\partial z}{\\partial w} = x$$\n",
    "\n",
    "### 4. The \"Magic\" Cancellation\n",
    "Multiplying them together:\n",
    "$$\\frac{\\partial L}{\\partial w} = \\left[ \\frac{\\hat{y}-y}{\\hat{y}(1-\\hat{y})} \\right] \\cdot \\left[ \\hat{y}(1-\\hat{y}) \\right] \\cdot x$$\n",
    "The terms $\\hat{y}(1-\\hat{y})$ cancel out perfectly, leaving:\n",
    "$$\\frac{\\partial L}{\\partial w} = (\\hat{y} - y)x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "887b3fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def update_weights(X, y_true, y_pred, weights, bias, learning_rate):\n",
    "    \"\"\"\n",
    "    Updates the model parameters using Gradient Descent.\n",
    "    \n",
    "    Parameters:\n",
    "    X (ndarray): Feature matrix of shape (n_samples, n_features)\n",
    "    y_true (ndarray): True labels of shape (n_samples, 1)\n",
    "    y_pred (ndarray): Predicted probabilities of shape (n_samples, 1)\n",
    "    weights (ndarray): Current weight vector\n",
    "    bias (float): Current bias scalar\n",
    "    learning_rate (float): The step size alpha\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (updated_weights, updated_bias)\n",
    "    \"\"\"\n",
    "    n = len(y_true)\n",
    "    \n",
    "    # Calculate the error (difference between prediction and truth)\n",
    "    error = y_pred - y_true\n",
    "    \n",
    "    # Calculate gradients\n",
    "    # dw: average of (error * features)\n",
    "    # X.T dot error performs the summation and multiplication efficiently\n",
    "    dw = (1 / n) * np.dot(X.T, error)\n",
    "    \n",
    "    # db: average of the error\n",
    "    db = (1 / n) * np.sum(error)\n",
    "    \n",
    "    # Parameter update rule: theta = theta - learning_rate * gradient\n",
    "    new_weights = weights - learning_rate * dw\n",
    "    new_bias = bias - learning_rate * db\n",
    "    \n",
    "    return new_weights, new_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f94fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.loss_history = []\n",
    "    \n",
    "    def stable_sigmoid(self, z):\n",
    "        # For z >= 0: 1 / (1 + exp(-z))\n",
    "        # For z < 0: exp(z) / (1 + exp(z))\n",
    "        # This ensures that exp() is only called with non-positive arguments.\n",
    "        return np.where(z >= 0, 1 / (1 + np.exp(-z)), np.exp(z) / (1 + np.exp(z)))\n",
    "\n",
    "    def binary_cross_entropy(self, y_true, y_pred):\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        loss = -(y_true * np.log(y_pred) + (1-y_true) * np.log(1 - y_pred))\n",
    "        return np.mean(loss)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # Gradient descent loop\n",
    "        for _ in range(self.n_iterations):\n",
    "            # Step1: Calculate predicted values\n",
    "            y_predicted = self.stable_sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "\n",
    "            # Step2: Calculate gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            # Step3: Update weights and bias\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "            # Step4: Calculate and store loss\n",
    "            loss = self.binary_cross_entropy(y, y_predicted)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        y_predicted = self.stable_sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "        return np.where(y_predicted >= 0.5, 1, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
