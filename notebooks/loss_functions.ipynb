{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mean Squared Error (MSE)\n",
    "\n",
    "Mean Squared Error is typically used for **regression** tasks. It measures the average of the squares of the errorsâ€”that is, the average squared difference between the estimated values ($\\hat{y}$) and the actual value ($y$).\n",
    "\n",
    "The formula for MSE is:\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "### Implementation Task:\n",
    "Implement a function `mean_squared_error` that:\n",
    "1. Takes two NumPy arrays: `y_true` and `y_pred`.\n",
    "2. Returns a single scalar representing the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the Mean Squared Error (MSE) between true labels and predictions.\n",
    "\n",
    "    Args:\n",
    "        y_true: Ground truth values.\n",
    "        y_pred: Predicted values from the model.\n",
    "    \"\"\"\n",
    "    # Return the mean of squared errors\n",
    "    return np.mean((y_true - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Binary Cross-Entropy (BCE)\n",
    "\n",
    "Binary Cross-Entropy is the standard loss function for **binary classification**. It penalizes the model based on how far the predicted probability is from the actual label (0 or 1). \n",
    "\n",
    "If $y = 1$, the loss is $-\\log(\\hat{y})$. If $y = 0$, the loss is $-\\log(1 - \\hat{y})$.\n",
    "\n",
    "While the loss for a single point is $L$, we minimize the average loss over $N$ samples (the Cost Function $J$):\n",
    "\n",
    "$$J = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)]$$\n",
    "\n",
    "This ensures that our loss value is scale-invariant (it doesn't double just because you doubled your batch size).\n",
    "\n",
    "### Implementation Task:\n",
    "Implement a function `binary_cross_entropy` that:\n",
    "1. Takes `y_true` (actual labels) and `y_pred` (predicted probabilities).\n",
    "2. Uses a small epsilon ($\\epsilon = 1e-15$) to clip `y_pred` to avoid `log(0)` errors.\n",
    "3. Returns the average loss over all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the Binary Cross-Entropy (BCE) between true labels and predictions.\n",
    "\n",
    "    Args:\n",
    "        y_true: Ground truth values.\n",
    "        y_pred: Predicted values from the model.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15\n",
    "    # np.clip:Clip array elements to the specified min and max range\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon) \n",
    "    loss = -(y_true * np.log(y_pred) + (1-y_true) * np.log(1 - y_pred))\n",
    "    # return the average loss over the batch\n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Use Cross-Entropy instead of MSE for Classification?\n",
    "\n",
    "While Mean Squared Error (MSE) is the go-to for regression, it is rarely used for classification tasks involving probabilities. Here are the two primary reasons:\n",
    "\n",
    "### 1. The Vanishing Gradient Problem (Learning Speed)\n",
    "When using a Sigmoid activation function with MSE, the gradient becomes very small when the prediction is \"very wrong\" (e.g., predicting 0.99 for a label of 0). This leads to extremely slow convergence. Cross-Entropy's derivative cancels out the Sigmoid derivative's \"flatness,\" ensuring a strong gradient when the error is high.\n",
    "\n",
    "### 2. Non-Convexity\n",
    "For logistic regression, MSE results in a **non-convex** loss surface with many local minima. Cross-Entropy, however, is **convex**, guaranteeing that gradient descent can find the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 0.9801\n",
      "BCE Loss: 4.6052\n"
     ]
    }
   ],
   "source": [
    "# Test Case: A \"very wrong\" prediction\n",
    "y_true = np.array([1])\n",
    "y_pred = np.array([0.01]) # The model is 99% sure it's class 0, but it's actually 1\n",
    "\n",
    "mse_val = mean_squared_error(y_true, y_pred)\n",
    "bce_val = binary_cross_entropy(y_true, y_pred)\n",
    "\n",
    "print(f\"MSE Loss: {mse_val:.4f}\") \n",
    "# MSE is bounded; even a total failure results in a max loss of 1.0\n",
    "\n",
    "print(f\"BCE Loss: {bce_val:.4f}\") \n",
    "# BCE is logarithmic; as the error approaches 1, the loss approaches infinity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Insight: Gradient Saturation\n",
    "\n",
    "In classification, we usually use the **Sigmoid** activation function:\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "The derivative of MSE with respect to the weights involves $\\sigma'(z)$. As $\\sigma(z)$ approaches 0 or 1 (the \"flat\" regions of the curve), $\\sigma'(z)$ becomes nearly zero. This causes the weights to stop updating, a phenomenon known as **gradient saturation**.\n",
    "\n",
    "![pic](../assets/Sigmoid_function_and_its_derivative.png)\n",
    "\n",
    "Cross-Entropy solves this. When we take the derivative of the BCE loss with respect to the weights, the denominator from the Sigmoid derivative is canceled out. This leaves a gradient that is proportional to the linear error $(y - \\hat{y})$, meaning the model learns faster when it is further from the truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why BCE is Superior: Gradient Analysis\n",
    "\n",
    "To understand why BCE is preferred over MSE for classification, we must look at the gradients with respect to the weights $w$ when using a **Sigmoid** activation function: $\\hat{y} = \\sigma(z)$, where $z = wx + b$.\n",
    "\n",
    "The derivative of the Sigmoid function is:\n",
    "$$\\sigma'(z) = \\sigma(z)(1 - \\sigma(z)) = \\hat{y}(1 - \\hat{y})$$\n",
    "\n",
    "\n",
    "\n",
    "[Image of Sigmoid function and its derivative]\n",
    "\n",
    "\n",
    "### Case 1: Mean Squared Error (MSE)\n",
    "The loss for one sample is $L = \\frac{1}{2}(y - \\hat{y})^2$. Using the chain rule:\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}$$\n",
    "$$\\frac{\\partial L}{\\partial w} = -(y - \\hat{y}) \\cdot \\sigma'(z) \\cdot x$$\n",
    "$$\\frac{\\partial L}{\\partial w} = -(y - \\hat{y}) \\cdot \\hat{y}(1 - \\hat{y}) \\cdot x$$\n",
    "\n",
    "**The Problem (Gradient Vanishing):** If the prediction $\\hat{y}$ is very close to $0$ or $1$ (even if it's the **wrong** prediction), the term $\\hat{y}(1 - \\hat{y})$ becomes extremely small (near $0$). This \"kills\" the gradient, and the model stops learning.\n",
    "\n",
    "### Case 2: Binary Cross-Entropy (BCE)\n",
    "The loss is $L = -[y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})]$. Using the chain rule:\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}$$\n",
    "$$\\frac{\\partial L}{\\partial w} = -(\\frac{y}{\\hat{y}} - \\frac{1-y}{1-\\hat{y}}) \\cdot \\hat{y}(1 - \\hat{y}) \\cdot x$$\n",
    "Simplifying the term inside the parenthesis:\n",
    "$$\\frac{\\partial L}{\\partial w} = -(\\frac{y(1-\\hat{y}) - \\hat{y}(1-y)}{\\hat{y}(1-\\hat{y})}) \\cdot \\hat{y}(1 - \\hat{y}) \\cdot x$$\n",
    "Notice how the denominator $\\hat{y}(1-\\hat{y})$ cancels out the Sigmoid derivative!\n",
    "\n",
    "**The Result:**\n",
    "$$\\frac{\\partial L}{\\partial w} = (\\hat{y} - y)x$$\n",
    "\n",
    "**The Advantage:**\n",
    "The gradient is purely proportional to the error $(\\hat{y} - y)$. There is no \"vanishing\" term. If the error is large, the gradient is large, and the model learns quickly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
