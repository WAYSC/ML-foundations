{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics Basics: Manual Implementation\n",
    "\n",
    "## Part 1: Numpy Refresher & Data Generation\n",
    "\n",
    "Before diving into statistics, let's establish a synthetic dataset using Numpy. We will use this dataset to verify our manual implementations against Numpy's built-in functions.\n",
    "\n",
    "**Objectives:**\n",
    "1.  Initialize the random number generator for reproducibility.\n",
    "2.  Create a 1D array of random integers.\n",
    "3.  Inspect the array's properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51 92 14 71 60 20 82 86 74 74]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Set the seed for reproducibility (using Numpy's random state)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 2. Generate 10 random integers between 0 (inclusive) and 100 (exclusive)\n",
    "# np.random.randint(low, high, size)\n",
    "data = np.random.randint(0, 100, 10)\n",
    "\n",
    "# 3. Print the array and its attributes\n",
    "print(\"Data array:\", data)\n",
    "print(\"Shape:\", data.shape)\n",
    "print(\"Data type:\", data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Arithmetic Mean\n",
    "\n",
    "The arithmetic mean represents the central tendency of the data. It is the sum of all values divided by the number of observations.\n",
    "\n",
    "**Formula:**\n",
    "$$\\mu = \\frac{1}{N} \\sum_{i=1}^{N} x_i$$\n",
    "\n",
    "**Task:** Implement the mean calculation manually and compare it with Numpy's optimized implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy Mean: 62.4\n",
      "Manual Mean: 62.4\n"
     ]
    }
   ],
   "source": [
    "def calculate_mean(arr):\n",
    "    \"\"\"\n",
    "    Calculates the arithmetic mean of a numpy array.\n",
    "    \"\"\"\n",
    "    # Sum all elements\n",
    "    total = np.sum(arr)\n",
    "    # Count number of elements (arr.size is more 'numpy-thonic' than len())\n",
    "    count = arr.size\n",
    "    return total / count\n",
    "\n",
    "# Calculate using both methods\n",
    "np_mean = np.mean(data)\n",
    "cal_mean = calculate_mean(data)\n",
    "\n",
    "print(f\"Numpy Mean: {np_mean}\")\n",
    "print(f\"Manual Mean: {cal_mean}\")\n",
    "\n",
    "# Verification: assert that the values are close enough (floating point safety)\n",
    "assert np.isclose(np_mean, cal_mean), \"Mean calculation is incorrect!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Property/Function | Description | Return Type | Core Meaning |\n",
    "|-------------------|-------------|-------------|--------------|\n",
    "| arr.shape | View the specific size of each dimension | Tuple | Describes the **distribution** of array dimensions |\n",
    "| arr.size | Calculate the total number of elements across all dimensions | Integer | Describes the total **quantity** of array elements |\n",
    "| len(arr) | Get the length of the first dimension | Integer | Describes the **outer scale** of the array |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Variance\n",
    "\n",
    "Variance measures the spread or dispersion of the data points around the mean.\n",
    "\n",
    "**Key Concepts:**\n",
    "1.  **Squared Deviations:** We square the difference between each point and the mean so negative and positive differences don't cancel each other out.\n",
    "2.  **Bessel's Correction (ddof):**\n",
    "    * **Population Variance ($N$):** Used when we have data for the *entire* population.\n",
    "    * **Sample Variance ($N-1$):** Used when we only have a *sample* of the data. Dividing by $N-1$ provides an unbiased estimate.\n",
    "\n",
    "**Formulas:**\n",
    "$$\\sigma^2 = \\frac{\\sum (x_i - \\mu)^2}{N - ddof}$$\n",
    "* Where $ddof=0$ for Population.\n",
    "* Where $ddof=1$ for Sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy variance: 643.6400000000001\n",
      "Manual variance: 643.6400000000001\n",
      "Numpy variance: 715.1555555555557\n",
      "Manual variance: 715.1555555555557\n"
     ]
    }
   ],
   "source": [
    "def calculate_variance(arr, ddof = 0):\n",
    "    \"\"\"\n",
    "    Calculates the variance of array arr.\n",
    "    ddof = 0 for Population Variance.\n",
    "    ddof = 1 for Sample Variance.\n",
    "    \"\"\"\n",
    "    # 1. Calculate Mean\n",
    "    arr_mean = np.mean(arr)\n",
    "    # 2. Squared Deviations (Vectorized)\n",
    "    square_arr = (arr - arr_mean) ** 2\n",
    "    # 3. Sum of Squared Deviations\n",
    "    total = np.sum(square_arr)\n",
    "    # 4. Division (Fixing the denominator logic)\n",
    "    n = arr.size\n",
    "    return total / (n - ddof)\n",
    "\n",
    "# --- Verification ---\n",
    "\n",
    "# Test 1: Popolation Variance\n",
    "np_var = np.var(data)\n",
    "cal_var = calculate_variance(data)\n",
    "\n",
    "print(f\"Numpy variance: {np_var}\")\n",
    "print(f\"Manual variance: {cal_var}\")\n",
    "\n",
    "# Verification: assert that the values are close enough (floating point safety)\n",
    "assert np.isclose(np_var, cal_var), \"Variance calculation is incorrect!\"\n",
    "\n",
    "# Test 2: Sample Variance\n",
    "np_var = np.var(data, ddof=1)\n",
    "cal_var = calculate_variance(data, 1)\n",
    "\n",
    "print(f\"Numpy variance: {np_var}\")\n",
    "print(f\"Manual variance: {cal_var}\")\n",
    "\n",
    "# Verification: assert that the values are close enough (floating point safety)\n",
    "assert np.isclose(np_var, cal_var), \"Variance calculation is incorrect!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ddof Value | Denominator | Variance Type | Core Description | Key Usage Rule |\n",
    "|------------|-------------|---------------|------------------|----------------|\n",
    "| 0          | n           | Population Variance | Reflects the true dispersion of the **complete population**, unbiased | Use for full population data |\n",
    "| 1          | n-1         | Sample Variance | Unbiased estimate of population variance via sample, avoids underestimation with df correction | Use for sampled data from population |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we built the foundational building blocks of statistics from scratch using Python and Numpy.\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1.  **Numpy Efficiency:** We learned how to generate synthetic data using `np.random` and why Numpy's vectorized operations are preferred over loops.\n",
    "2.  **Arithmetic Mean:**\n",
    "    * **Concept:** The central tendency of the data.\n",
    "    * **Implementation:** Summing all elements and dividing by the count ($N$).\n",
    "3.  **Variance:**\n",
    "    * **Concept:** The measure of how spread out the data is.\n",
    "    * **The Critical Distinction:** We implemented the `ddof` (Delta Degrees of Freedom) parameter to distinguish between:\n",
    "        * **Population Variance (`ddof=0`):** Dividing by $N$. Used when we have the complete dataset.\n",
    "        * **Sample Variance (`ddof=1`):** Dividing by $N-1$ (Bessel's Correction). Used to estimate population variance from a sample.\n",
    "\n",
    "**Verification:**\n",
    "We successfully verified that our manual implementations match `np.mean()` and `np.var()` results, confirming our understanding of the underlying mathematics.\n",
    "\n",
    "---\n",
    "\n",
    "## Bonus: Performance Benchmark\n",
    "\n",
    "Just how much faster is Numpy? Let's compare a pure Python loop implementation against Numpy's vectorized implementation on a larger dataset (1,000,000 elements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pure Python Time: 0.24327 seconds\n",
      "Numpy Time:       0.00242 seconds\n",
      "\n",
      ">>> Numpy is 100.5x faster!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# 1. Generate a large dataset\n",
    "large_data = np.random.randn(1000000)\n",
    "\n",
    "# 2. Pure Python Implementation (Simulating \"Slow\" Code)\n",
    "def python_variance(arr):\n",
    "    # Note: This is slow because it iterates element by element in Python\n",
    "    n = len(arr)\n",
    "    m = sum(arr) / n\n",
    "    return sum((x - m)**2 for x in arr) / n\n",
    "\n",
    "# 3. Benchmark Python\n",
    "start_time = time.time()\n",
    "python_variance(large_data)\n",
    "python_time = time.time() - start_time\n",
    "print(f\"Pure Python Time: {python_time:.5f} seconds\")\n",
    "\n",
    "# 4. Benchmark Numpy\n",
    "start_time = time.time()\n",
    "np.var(large_data)\n",
    "numpy_time = time.time() - start_time\n",
    "print(f\"Numpy Time:       {numpy_time:.5f} seconds\")\n",
    "\n",
    "# 5. Result\n",
    "print(f\"\\n>>> Numpy is {python_time / numpy_time:.1f}x faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Core Reason | Detailed Explanation | Key Advantage |\n",
    "|-------------|----------------------|---------------|\n",
    "| **Vectorized Operations** | NumPy abandons Python's one-by-one element loop, and performs mathematical operations on the **entire array** at once. It avoids the huge overhead of loop iteration in Python. | Eliminates Python loop iteration overhead, one operation for all elements |\n",
    "| **Implemented in Low-Level Language** | NumPy's core calculation logic is written in **C/Fortran** (compiled languages), which runs directly on the CPU with high execution efficiency. Python loops are interpreted and executed line by line with low efficiency. | Compiled execution (C/Fortran) vs Python's interpreted execution, much lower CPU execution overhead |\n",
    "| **Contiguous Memory Layout** | NumPy arrays store data in **contiguous physical memory blocks**, which can fully utilize CPU cache (cache hit rate is extremely high). Python lists store scattered object references, with frequent memory access and low cache utilization. | High CPU cache hit rate, reduces memory access time |\n",
    "| **Broadcast Mechanism** | For operations between arrays of different shapes (e.g., array - constant), NumPy automatically expands the small operand to match the shape of the large operand **without copying data**, avoiding the memory overhead of manual loop expansion. | No redundant data copying during heterogeneous operations, saves memory and time |\n",
    "| **Avoid Python Object Overhead** | NumPy arrays store **homogeneous basic data types** (e.g., int32/float64), without the extra overhead of Python's built-in object packaging (e.g., int/float objects). Python loops process packaged objects with high per-element overhead. | Reduces per-element processing overhead, more efficient data storage |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
